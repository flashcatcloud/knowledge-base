---
title: "RUM Issue Alerting"
description: "Learn how Flashduty RUM Issues trigger alerts"
date: "2024-05-20T10:00:00+08:00"
url: "https://docs.flashcat.cloud/en/flashduty/rum/issue-alerting?nav=01JCQ7A4N4WRWNXW8EWEHXCMF5"
---

## Overview

Flashduty RUM automatically aggregates all error events reported by the SDK into Issues, helping you prioritize and identify the most impactful problems, making it easier to reduce service downtime and user frustration.

You can perform daily inspections of aggregated Issues in the console, or configure alert notifications for Issues to be notified the moment problems occur.

## Enabling Alerts

Navigate to the `Application Details` - `Alert Settings` page to enable alerts. You can choose to deliver alerts to multiple collaboration spaces. Alert notification rules follow the dispatch strategy under the collaboration space. You can set up on-call personnel for your team, so alerts will be assigned to the person on duty when they occur.

![2025-05-20-10-53-57](https://docs-cdn.flashcat.cloud/images/png/2bbd455a2ac702246e8399b6628f9158.png)

:::tip
You must activate the On-call service to enable Issue alerts. Note that the On-call service charges based on active users, but members without licenses can still receive alert notifications, as even the free version has basic notification capabilities.
:::

## When Alerts Trigger

1. **New Issues**: When error events cause new Issues to appear, an alert event is triggered.
2. **Issue Updates**: When error events continuously merge into an unclosed Issue (pending, in progress), and it's been more than 24 hours since the last alert event, a new alert event will be triggered.
3. **Issue Reopening**: When new errors are merged into a closed Issue, causing it to reopen, i.e., a regression.

Issues trigger alert events that are delivered to collaboration spaces. Whether alert notifications are triggered depends on your integration settings, noise reduction configurations, and dispatch strategy configurations in the collaboration space. Please read the On-call service documentation for details.

When an Issue is closed, the system triggers a closure-type alert event, and its associated incident may automatically recover.

## Alert Severity

The severity of alert events triggered by Issues is currently automatically generated by the system. This choice balances configuration complexity and rationality, and may change in the future. The current system judgment rules are as follows:

1. **Basic Judgment**:

   - If an Issue has existed for more than 7 days, it is directly classified as "Info" level
   - If it's a crash issue, it is directly classified as "Critical" level

2. **Scoring System**: Severity is determined by accumulated scores, with final scores:

   - ≥70 points: Critical
   - ≥40 points: Warning
   - <40 points: Info

3. **Scoring Factors**:

   - Environment impact: Production environment (50 points), Pre-production environment (30 points), Other environments (10 points)
   - Error keywords: Contains critical keywords (+30 points) or warning keywords (+15 points)
   - Suspected causes: API failure (+20 points), Code exception (+15 points), Unknown/Network error (+5 points)
   - Problem duration: More than 24 hours (+20 points), More than 12 hours (+10 points) 