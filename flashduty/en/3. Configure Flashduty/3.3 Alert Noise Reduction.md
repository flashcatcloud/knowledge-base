---
title: "Configure Alert Noise Reduction"
description: "Through alert grouping, multiple similar active alerts can be grouped into a single incident for unified assignment, notification, and handling, significantly reducing notification frequency and improving response efficiency"
date: "2024-06-18T10:00:00+08:00"
url: "https://docs.flashcat.cloud/en/flashduty/noise-reduction-settings"
---

## Video Guide
---
<Video src="https://download.flashcat.cloud/flashduty/video/alert-aggr.mp4"></Video>


## Configure Alert Grouping
---
Go to [Channel Details] - [Reduce Noise] to set up **alert grouping** policies. When creating a new channel, alert grouping is disabled by default. We recommend enabling it manually and configuring grouping strategies as needed.

:::tip
When alert grouping is disabled, each alert will create a separate incident with identical basic information.
:::

- Grouping Dimensions: A channel can have multiple default grouping dimensions. Alerts are considered similar and can be grouped if they match any dimension set.

  - Enable `Fine-grained Control` if you want to handle different alerts separately.
  - Fine-grained control allows you to filter incidents and set specific grouping dimensions.
  - The system always prioritizes fine-grained control matches; if no match is found, default grouping dimensions are used.
  - You can learn how to configure filtering conditions at [Configure Filtering Rules](https://docs.flashcat.cloud/en/flashduty/how-to-filter).

- Grouping Window: You can choose to group only alerts that occur within a specific timeframe (stronger correlation). Alerts outside this window will trigger new incidents. **Note that this is a sliding window that extends with each new grouped alert**.

  - Generally, we recommend setting the grouping window based on the average alert arrival time, such as 10 minutes.

- Storm Warning: After an incident is triggered, the system will immediately assign and notify (unless delayed notification is set). Subsequent alerts will be grouped without triggering new notifications, which may prevent timely awareness of alert storms. Therefore, we provide this threshold - when the number of grouped alerts reaches the threshold, the system will trigger a storm warning to remind you to handle it urgently.

  - We always recommend enabling storm warnings.

- Preview: You can use the preview feature to fetch recent events and render noise reduction results in real-time to evaluate effectiveness. The system fetches up to `666` historical events.

<img src="https://api.apifox.com/api/v1/projects/4386769/resources/448371/image-preview" style="display: block; margin: 0 auto;" height="400" />

## Grouping Example
---

With a channel configured to group by **alert check item**, the system receives 5 alert notifications that trigger alerts and incidents in sequence:

```
Incident: cpu idle < 20% / es.nj.03, Critical

  - Alert cpu idle < 20% / es.nj.03:
      - Event 1: es.nj.03, cpu.idle = 10%, Critical
      - Event 2: es.nj.03, cpu.idle = 18%, Warning
      - Event 4: es.nj.03, cpu.idle = 10%, Ok

  - Alert cpu idle < 20% / es.nj.01:
      - Event 3: es.nj.01, cpu.idle = 15%, Warning
  
  - Alert cpu idle < 20% / es.nj.02:
      - Event 5: es.nj.02, cpu.idle = 19%, Warning
```

Through the console's incident details page, we can see the final [Incident-Alert-Event] relationships:
- Click the alert title to view associated alert details, including timeline and related events
- Click event points to view specific event content, including labels and descriptions

![image.png](https://api.apifox.com/api/v1/projects/4169655/resources/435540/image-preview)

## FAQ
---
<details>
  <summary>Does the incident title change when new alerts are grouped?</summary>
  No, by default, the incident title remains identical to the first alert that triggered it. You can manually modify the incident title at any time, and it won't change as new alerts are grouped.
</details>
<details>
  <summary>Do incident labels change when new alerts are grouped?</summary>
  
  - Manually created alerts: No, their label list will always remain empty
  - Automatically triggered alerts: Possibly. The incident labels will match the labels of the first alert that triggered it, and if the alert labels change, the incident labels will update accordingly.
</details>
<details>
  <summary>Do alert labels change when new events are grouped?</summary>
  Yes, alert labels always stay consistent with newly grouped events. For example, if you receive a "CPU idle too low" alert at 10:00 with a trigger value of 10%, this trigger value label may change dynamically as more events are grouped. However, if a new recovery event is received, the alert will maintain its existing labels and only add previously non-existent labels. Our principle is to maintain alert labels as close as possible to their trigger state.
</details>
<details>
  <summary>Is there a limit to the number of alerts that can be grouped into an incident?</summary>
  Yes, we limit each incident to grouping a maximum of 1000 alerts, primarily to reduce console page rendering time. However, as Flashduty is a high-performance event processing system with extensive backend concurrency logic, it's normal to occasionally see incidents with more than 1000 grouped alerts.
</details>
<details>
  <summary>Is there a limit to the number of events that can be grouped into an alert?</summary>
  No, but an alert's event grouping window is limited to 24 hours. This means if an alert hasn't recovered after 24 hours, no new events will be grouped into it. New events received by Flashduty will generate new alerts.
</details>
<details>
  <summary>Why doesn't the number of events I pushed match the number of events associated with the alert?</summary>
  Event-to-alert merging is also a noise reduction process. If Flashduty determines that a newly reported event doesn't significantly differ from the alert (e.g., no changes in status, severity, description, etc.), Flashduty will discard the new event and use its labels to override existing ones.
</details>